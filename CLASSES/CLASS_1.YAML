1) AIRFLOW:
  - it is a open source workflow management system
  - it can lead with erros, log, monitor and schedule workflows, repeat tasks, trigger alerts, etc.
  - its source is a dag (directed acyclic graph) which is a collection of all the tasks you want to run, 
    organized in a way that reflects their relationships and dependencies. it is a ELt tool.
  - task is a instace of an operator that is what is gonna be made in the dag
  - providers are the modules that are used in the operators to have access to other tools such as awd, databricks, mysql, etc.

  - installation:
    - install docker
    - install airflow with docker:
        - create a folder called airflow in your home directory
        - put the file docker-compose.yaml in the folder
        - .env file in the folder
         
        - in terminal:
          - cd airflow
          - docker-compose up -d # to start the container
          - docker-compose ps # to check all images running and installed in the container

        in the browser:
          - localhost:8080
          - login: airflow
          - password: airflow
  - how is a DAG in python:
      from airflow import DAG
      from airflow.operators.base_operator import BaseOperator
      from datetime import datetime, timedelta

      dag = DAG(
          'my_dag',
          default_args=default_args,
          description='A simple tutorial DAG',
          schedule_interval=timedelta(days=1),
          start_date=datetime(2021, 1, 1),
          catchup=False
      )

      task1 = BashOperator(
          task_id='task1',
          bash_command='echo "Hello World"',
          dag=dag
      )
      task2 = BashOperator(
          task_id='task2',
          bash_command='echo "Hello World"',
          dag=dag
      )
        
      task3 = BashOperator(
          task_id='task3',
          bash_command='echo "Hello World"',
          dag=dag
      )

      task1 >> task2 >> task3

  - setting the docker-compose:
    AIRFLOW__CORE__LOAD_EXAMPLES: False
    AIRFLOW__WEBSERVER__EXPOSE__CONFIG: 'true' # to expose the config
    AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: 5 # to set the interval of the scheduler to check the dags
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 20 # to set the interval of the scheduler to check the dags
  
  - docker-compose down # to stop the container
  - docker-compose up -d  # to start the container

2) type of operator:
  - bash operator: to run bash commands
  - python operator: to run python functions
  - http operator: to make http requests
  - email operator: to send emails
  - sql operator: to run sql commands
  - docker operator: to run docker commands
  - kubernetes operator: to run kubernetes commands
  - dummy operator: to create a dummy task
  - file sensor: to check if a file exists
  - http sensor: to check if a http request was successful
  - triggerDagRunOperator: to trigger another dag (execute another dag)
  - triggerRule: to trigger a dag based on a rule:
    - all_success # to trigger a dag if all tasks are successful
    - all_failed # to trigger a dag if all tasks are failed
    - one_failed # to trigger a dag if one task is failed
    - one_success # to trigger a dag if one task is successful
    - all_done # to trigger a dag if all tasks are done
    - none_failed # to trigger a dag if none task is failed
    - none_skipped # to trigger a dag if none task is skipped
    - dummy # to trigger a dag if a dummy task is done
  
3) parametrys of dag:
  - default_args: to set the default arguments of the dag for all the tasks
  - description: to set the description of the dag
  - schedule_interval: to set the interval of the dag:
    - timedelta(days=1)
    - cron: '0 0 * * *' # 0 minutes, 0 hours, every day, every month, every day of the week
    - @daily, @hourly, @weekly, @monthly, @yearly...
  - start_date: to set the start date of the dag
  - end_date: to set the end date of the dag
  - catchup: to set if the dag is gonna run all the tasks that were not run in the past
  - backfill: to set if the dag is gonna run all the tasks that were not run in the past but only once
  - max_active_runs: to set the max number of active runs of the dag
  - concurrency: to set the number of tasks that can run at the same time
  - depends_on_past: to set if the task is gonna run only if the last task was successful
  - retries: to set the number of retries of the task
  - retry_delay: to set the delay between the retries
  - retry_exponential_backoff: to set if the delay between the retries is gonna be exponential
  - email_on_failure: to set if the email is gonna be sent if the task fails
  - email_on_retry: to set if the email is gonna be sent if the task retries
  - email_on_success: to set if the email is gonna be sent if the task is successful

4) setting up gmail:
  - go to https://myaccount.google.com/
  - security
  - 2-step verification: on
  - app password: generate a password for the app
  - in the docker-compose.yaml:
      - AIRFLOW__SMTP__SMTP_USER: 'my email address'
      - AIRFLOW__SMTP__SMTP_PASSWORD: 'the password generated'
      - AIRFLOW__SMTP__SMTP_MAIL_FROM: 'Airflow'
      - AIRFLOW__SMTP__SMTP_PORT: 587
      - AIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com

  - docker-compose down
  - docker-compose up -d

5) task dummy:
  - when we have a list of tasks before and another one after, we can use the dummy task to connect them
  - [task1, task2, task3] >> dummy >> [task4, task5, task6]

6) pools:
  - it a way to limit the number of tasks that can run at the same time
  - example: if we have a database that can only handle 10 connections, we can set a pool with 10 connections
  - in the dag:
    - from airflow.models.pool import Pool
    - pool = Pool(
        pool='my_pool',
        slots=10,
        description='my pool'
      )
    - pool = Pool.get_pool('my_pool')
    - pool.slots

  - or define it in localhost:8080 -> Admin -> Pools

7) branchs:
  - it is a way to create a condition in the dag
  - create a dag with a branch operator:
    - from airflow.operators.python_operator import BranchPythonOperator
    - from airflow.operators.dummy_operator import python_operator

    - def branch_func(**kwargs):
        if True:
          return 'task1'
        else:
          return 'task2'
    - branch = BranchPythonOperator(
        task_id='branch',
        python_callable=branch_func,
        provide_context=True,
        dag=dag
      )
    - [task1, task2] >> branch >> [task3, task4]
